{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" librerías requeridas\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, metrics, model_selection\n",
    "from sklearn.linear_model import ElasticNet as Raso\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sacaríamos el dataframe desde el qeu trabajar\"\"\"\n",
    "df = pd.read_csv(r\"C:\\Users\\Victor\\Desktop\\Data2 - copia\\3-Machine_Learning\\1-Supervisado\\1-Regression\\2-Polynomial_Regression\\ejercicios\\data\\Boston.csv\",index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" vemos los nan y el tipo de datos\"\"\"\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" vemos las medias y sabemos si haría falta normalizar\"\"\"\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" vemos la correlación entre las columnas del df, para saber si hay algunas con correlación muy alta, en cuyo caso podríamos quitarlas del df\"\"\"\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.heatmap(df.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" quitamos aquellas que sean necesarias, en este caso, sin perderlas\"\"\"\n",
    "çdf1 = df.pop(\"tax\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NO NECESARIO, pero también podemos sacar gráficas para ver la relación entre variables\"\"\"\n",
    "sns.scatterplot(x = df.loc[:,\"lstat\"] ,y = df.loc[:,\"medv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" preparamos los datos para hacer el entrenamiento del modelo\"\"\"\n",
    "# LAS X TIENEN DOS COLUMNAS, LAS Y SON UNA SERIE \n",
    "X = df.loc[:,[\"lstat\",\"rm\"]]\n",
    "X.info()\n",
    "y = df.loc[:,\"medv\"]\n",
    "y.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generamos las variables para el entrenamiento, teniendo en cuenta el test_size(siempre en decimales, pues los toma como porcentuales, y el random_state, que debe ser siemre igual en nuestras pruebas)\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size= 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" confirmamos el correcto tamaño de las variables creadas\"\"\"\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(404, 2)\n",
    "(102, 2)\n",
    "(404,)\n",
    "(102,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" importamos la función de regresión final, y la declaramos en una variable\"\"\"\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ENTRENAMOS EL MODELO con las variables de entrenamiento\"\"\"\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" GENERAMOS LAS PREDICCI0NES EN BASE AL MODELO YA ENTRENADO con las variables de test\"\"\"\n",
    "predictions = lr.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" obtenemos el score de nuestro modelo\"\"\"\n",
    "lr.score(X_test,y_test)# esto equivale a un porcentual de acierto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" calculamos los diferentes tipos de error\"\"\"\n",
    "print('MAE train', mean_absolute_error(y_test, lr.predict(X_test)))\n",
    "print('MAPE train', mean_absolute_percentage_error(y_test, lr.predict(X_test)))\n",
    "print('MSE train', mean_squared_error(y_test, lr.predict(X_test)))\n",
    "print('RMSE train', np.sqrt(mean_squared_error(y_test, lr.predict(X_test))))\n",
    "print('R2 score train', r2_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IMPORTAMOS LA FUNCIÓN PARA GENERAR LOS POLYNOMIAL FEATURES, INICIALIZAMOS LA VARIABLE, Y DECIDIMOS EL GRADO CON EL QUE VAMOS A EMPEZAR A MODIFICAR LAS Xs \"\"\"\n",
    "# FIT Y TRANSFORM DEL TRAIN CON POLYNOMIALFEATURES DE GRADO 2 Y ENTRENAMOS EL MODELO CON ESOS DATOS, LO LLAMAMOS pol_reg\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg = PolynomialFeatures(degree = 2)# esto hace la elevación al cuadrado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generamos la variable con las nuevas Xs_train modificadas\"\"\"\n",
    "poly_reg.fit(X_train)# aquí solo estamos transformando la X al cuadrado (siempre en función del valor de degree)\n",
    "X_poly = poly_reg.transform(X_train)# definimos la nueva variable con las Xs transformadas\n",
    "X_poly# esto son las Xs originales elevadas al cuadrado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" hacemos la regresión lineal, ahora ya con las Xs modificadas\"\"\"\n",
    "poly_reg2 = LinearRegression()\n",
    "poly_reg2.fit(X_poly,y_train)# hacemos la regresión lineal, ahora con las Xs al cuadrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" calculamos las Xs_test modificadas\"\"\"\n",
    "poly_reg.fit(X_test)# aquí solo estamos transformando\n",
    "X_poly2 = poly_reg.transform(X_test)# hacemos la conversión de las Xs_test a grado 2. # variable para entrenar el modelo, viene de polinomial features\n",
    "y_pred = poly_reg2.predict(X_poly2)# resultado de las Ys en función de las Xs_test modificadas.# variable para calcular las ys de test, viene de la regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" obtenemos los errores, ahora con las Xs_test modificadas en grado 2\"\"\"\n",
    "\"\"\" utilizamos ya la transformada de grado 2\"\"\"\n",
    "print('MAE test', mean_absolute_error(y_test, y_pred))\n",
    "print('MAPE test', mean_absolute_percentage_error(y_test,y_pred))\n",
    "print('MSE test', mean_squared_error(y_test, y_pred))\n",
    "print('RMSE test', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('R2 score test', r2_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
